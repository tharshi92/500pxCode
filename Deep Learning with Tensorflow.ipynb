{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Let us first load the MNIST dataset\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "# Import modules\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create placeholders for the input images and the target classes\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "# Variables in our model (weights and biases)\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Initialize all tf variables\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is our regression model\n",
    "y = tf.matmul(x, W) + b\n",
    "\n",
    "# Define a loss function (cross entropy)\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model using gradient descent with a step size of 0.5\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# Run training step multiple times to reduce loss\n",
    "for i in range(int(1e3)):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    train_step.run(feed_dict={x: batch[0], y_:batch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9216\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "# Get percentage of correct results (accuracy)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# Print accuracy on test images\n",
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There is a small caveat here, ~92% correct is bad for a dataset as ubiquitous as MNIST. So we will now use a covolutional neural network to increase this percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create new weights and biases. Since we are going to have a lot of these,\n",
    "# it is nice to define functions to create them for us.\n",
    "\n",
    "def weight_variable(shape):\n",
    "    \"\"\"Create a weight matrix with shape=shape and slight noise to prevent a zero gradient\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"Create a bias vector with shape=shape and a slight offset to prevent dead neurons.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Convolution will be done with a stride of 1 zero-padding to produce a filtered image\n",
    "# of the same size as the input image. Again, we use functions for this to keep our\n",
    "# code clean.\n",
    "\n",
    "def conv2d(x, W):\n",
    "    \"\"\"Do a convolution of x with filter W. The stride is 1 and we have 'SAME' padding.\"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# We use 2x2 pooling to reduce the size of the filtered image.\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    \"\"\"Max pooling done to x with a 2x2 sweep\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First convolutional layer (Convolution + Pooling) with 32 filters\n",
    "# of size 5x5 and 32 biases\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# Reshape input image\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "# Apply con2d, add bias, nonlinearly transform with RELU\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# Second convolutional layer: 64 filters for each 5x5 patch\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fully connected layer with 1024 Neurons\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neuron Dropout to reduce overfitting\n",
    "\n",
    "# Probability of a neuron staying connected\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "# Use tensorflows in-built dropout function to mask neurons\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Readout Layer\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-72d7af0b1f00>:16 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "step 0, training accuracy 0.06\n",
      "step 100, training accuracy 0.86\n",
      "step 200, training accuracy 0.96\n",
      "step 300, training accuracy 0.82\n",
      "step 400, training accuracy 0.92\n",
      "step 500, training accuracy 0.94\n",
      "step 600, training accuracy 0.9\n",
      "step 700, training accuracy 0.96\n",
      "step 800, training accuracy 0.94\n",
      "step 900, training accuracy 0.98\n",
      "step 1000, training accuracy 0.98\n",
      "step 1100, training accuracy 0.92\n",
      "step 1200, training accuracy 0.96\n",
      "step 1300, training accuracy 0.94\n",
      "step 1400, training accuracy 1\n",
      "step 1500, training accuracy 0.98\n",
      "step 1600, training accuracy 0.94\n",
      "step 1700, training accuracy 0.98\n",
      "step 1800, training accuracy 0.98\n",
      "step 1900, training accuracy 1\n",
      "step 2000, training accuracy 0.92\n",
      "step 2100, training accuracy 0.96\n",
      "step 2200, training accuracy 0.98\n",
      "step 2300, training accuracy 1\n",
      "step 2400, training accuracy 0.98\n",
      "step 2500, training accuracy 0.98\n",
      "step 2600, training accuracy 1\n",
      "step 2700, training accuracy 0.96\n",
      "step 2800, training accuracy 1\n",
      "step 2900, training accuracy 1\n",
      "step 3000, training accuracy 0.98\n",
      "step 3100, training accuracy 0.98\n",
      "step 3200, training accuracy 0.98\n",
      "step 3300, training accuracy 1\n",
      "step 3400, training accuracy 0.98\n",
      "step 3500, training accuracy 0.96\n",
      "step 3600, training accuracy 0.98\n",
      "step 3700, training accuracy 0.98\n",
      "step 3800, training accuracy 1\n",
      "step 3900, training accuracy 1\n",
      "step 4000, training accuracy 0.94\n",
      "step 4100, training accuracy 0.98\n",
      "step 4200, training accuracy 0.96\n",
      "step 4300, training accuracy 0.98\n",
      "step 4400, training accuracy 1\n",
      "step 4500, training accuracy 1\n",
      "step 4600, training accuracy 0.96\n",
      "step 4700, training accuracy 0.96\n",
      "step 4800, training accuracy 1\n",
      "step 4900, training accuracy 0.98\n",
      "step 5000, training accuracy 1\n",
      "step 5100, training accuracy 1\n",
      "step 5200, training accuracy 1\n",
      "step 5300, training accuracy 1\n",
      "step 5400, training accuracy 1\n",
      "step 5500, training accuracy 1\n",
      "step 5600, training accuracy 0.96\n",
      "step 5700, training accuracy 0.98\n",
      "step 5800, training accuracy 0.98\n",
      "step 5900, training accuracy 0.98\n",
      "step 6000, training accuracy 1\n",
      "step 6100, training accuracy 0.98\n",
      "step 6200, training accuracy 0.98\n",
      "step 6300, training accuracy 0.96\n",
      "step 6400, training accuracy 1\n",
      "step 6500, training accuracy 0.94\n",
      "step 6600, training accuracy 0.98\n",
      "step 6700, training accuracy 1\n",
      "step 6800, training accuracy 1\n",
      "step 6900, training accuracy 1\n",
      "step 7000, training accuracy 0.94\n",
      "step 7100, training accuracy 1\n"
     ]
    }
   ],
   "source": [
    "# Train and Evaluate The Model\n",
    "\n",
    "# Loss function (cross entropy)\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "\n",
    "# Training step using ADAM optimizer\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# Mask of correct predictions\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Run session\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Train over many steps\n",
    "for i in range(20000):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i%100 == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "# Print accuracy\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
